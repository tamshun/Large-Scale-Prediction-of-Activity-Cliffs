{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import optuna\n",
    "import torch\n",
    "from torch                             import nn\n",
    "from torch.utils.data                  import dataloader, Subset, DataLoader\n",
    "#from torch_geometric.data              import DataLoader\n",
    "from Tools.ReadWrite                   import ToJson, LoadJson\n",
    "from collections                       import defaultdict\n",
    "from sklearn.metrics                   import roc_auc_score\n",
    "from sklearn.model_selection           import StratifiedKFold\n",
    "from sklearn.metrics                   import roc_auc_score, accuracy_score\n",
    "from BaseFunctions_NN                  import Base_wodirection_CGR\n",
    "import random\n",
    "from functools import partial\n",
    "from rdkit import Chem\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "ELEM_LIST = list(range(1,119))\n",
    "ATOM_FDIM, BOND_FDIM = len(ELEM_LIST) + 21, 11\n",
    "\n",
    "\n",
    "def torch2numpy(x):\n",
    "    return x.to(\"cpu\").detach().numpy().copy()\n",
    "\n",
    "def onek_encoding_unk(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return [x == s for s in allowable_set]\n",
    "\n",
    "def atom_features(atom):\n",
    "    return onek_encoding_unk(atom.GetAtomicNum() , ELEM_LIST) + onek_encoding_unk(atom.GetDegree(), [0, 1, 2, 3, 4, 5])+ onek_encoding_unk(atom.GetFormalCharge(), [-1, -2, 1, 2, 0]) + onek_encoding_unk(int(atom.GetChiralTag()), [0, 1, 2, 3])+onek_encoding_unk(int(atom.GetHybridization()),[\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2\n",
    "    ])+[1 if atom.GetIsAromatic() else 0]  \n",
    "\n",
    "def bond_features(bond):\n",
    "    bt = bond.GetBondType()\n",
    "    stereo = int(bond.GetStereo())\n",
    "    fbond = [bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE, bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC, bond.IsInRing()]\n",
    "    fstereo = onek_encoding_unk(stereo, [0,1,2,3,4,5])\n",
    "    fbond=fbond + fstereo\n",
    "    return fbond\n",
    "\n",
    "def get_atom(graph, atom_num):\n",
    "    for atom in graph.GetAtoms():\n",
    "        if atom.GetIdx() == atom_num:\n",
    "            return atom\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "def index_select_ND(source: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Selects the message features from source corresponding to the atom or bond indices in index.\n",
    "    :param source: A tensor of shape (num_bonds, hidden_size) containing message features.\n",
    "    :param index: A tensor of shape (num_atoms/num_bonds, max_num_bonds) containing the atom or bond\n",
    "    indices to select from source.\n",
    "    :return: A tensor of shape (num_atoms/num_bonds, max_num_bonds, hidden_size) containing the message\n",
    "    features corresponding to the atoms/bonds specified in index.\n",
    "    \"\"\"\n",
    "    # source = source.long()\n",
    "    source     = source.float()\n",
    "    index      = index.long()\n",
    "    index_size = index.size()\n",
    "    suffix_dim = source.size()[1:]\n",
    "    final_size = index_size + suffix_dim\n",
    "    target     = source.index_select(dim=0, index=index.view(-1))\n",
    "    target     = target.view(final_size)\n",
    "    \n",
    "    return target\n",
    "\n",
    "\n",
    "class MolGraph:\n",
    "    \"\"\"\n",
    "    A MolGraph represents the graph structure and featurization of a single molecule.\n",
    "    A MolGraph computes the following attributes:\n",
    "    - smiles: Smiles string.\n",
    "    - n_atoms: The number of atoms in the molecule.\n",
    "    - n_bonds: The number of bonds in the molecule.\n",
    "    - f_atoms: A mapping from an atom index to a list atom features.\n",
    "    - f_bonds: A mapping from a bond index to a list of bond features.\n",
    "    - a2b: A mapping from an atom index to a list of incoming bond indices.\n",
    "    - b2a: A mapping from a bond index to the index of the atom the bond originates from.\n",
    "    - b2revb: A mapping from a bond index to the index of the reverse bond.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smiles: str, args=None, role=None):\n",
    "        \"\"\"\n",
    "        Computes the graph structure and featurization of a molecule.\n",
    "        :param smiles: A smiles string.\n",
    "        :param args: Arguments.\n",
    "        \"\"\"\n",
    "        self.smiles = smiles\n",
    "        self.n_atoms, self.n_bonds = 0, 0\n",
    "        self.f_atoms, self.f_bonds = [], []\n",
    "        self.a2b, self.b2a, self.b2revb = [], [], []\n",
    "        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        self.n_atoms = mol.GetNumAtoms()\n",
    "        for i, atom in enumerate(mol.GetAtoms()):\n",
    "            \n",
    "            self.f_atoms.append(atom_features(atom))\n",
    "            \n",
    "        self.f_atoms = [self.f_atoms[i] for i in range(self.n_atoms)]\n",
    "        for _ in range(self.n_atoms):\n",
    "            self.a2b.append([])\n",
    "\n",
    "        for a1 in range(self.n_atoms):\n",
    "            for a2 in range(a1 + 1, self.n_atoms):\n",
    "                bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                if bond is None:\n",
    "                    continue\n",
    "\n",
    "                f_bond = bond_features(bond)\n",
    "                self.f_bonds.append(f_bond)\n",
    "                self.f_bonds.append(f_bond)\n",
    "                b1 = self.n_bonds\n",
    "                b2 = b1 + 1\n",
    "\n",
    "                self.a2b[a2].append(b1)\n",
    "                self.b2a.append(a1)\n",
    "                self.a2b[a1].append(b2)\n",
    "                self.b2a.append(a2)\n",
    "                self.b2revb.append(b2)\n",
    "                self.b2revb.append(b1)\n",
    "                self.n_bonds += 2\n",
    "            \n",
    "\n",
    "\n",
    "class BatchMolGraph:\n",
    "    \"\"\"\n",
    "    A BatchMolGraph represents the graph structure and featurization of a batch of molecules.\n",
    "    A BatchMolGraph contains the attributes of a MolGraph plus:\n",
    "    - smiles_batch: A list of smiles strings.\n",
    "    - n_mols: The number of molecules in the batch.\n",
    "    - atom_fdim: The dimensionality of the atom features.\n",
    "    - bond_fdim: The dimensionality of the bond features (technically the combined atom/bond features).\n",
    "    - a_scope: A list of tuples indicating the start and end atom indices for each molecule.\n",
    "    - b_scope: A list of tuples indicating the start and end bond indices for each molecule.\n",
    "    - max_num_bonds: The maximum number of bonds neighboring an atom in this batch.\n",
    "    - b2b: (Optional) A mapping from a bond index to incoming bond indices.\n",
    "    - a2a: (Optional): A mapping from an atom index to neighboring atom indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mol_graphs: list):\n",
    "        \n",
    "        #self.args = args\n",
    "        self.masks = []\n",
    "        self.smiles_batch = []\n",
    "        self.mol_graphs = mol_graphs\n",
    "\n",
    "        self.atom_fdim = ATOM_FDIM\n",
    "        self.bond_fdim = BOND_FDIM \n",
    "\n",
    "        self.n_atoms = 1\n",
    "        self.n_bonds = 1\n",
    "\n",
    "        f_atoms = [[0] * self.atom_fdim]\n",
    "        f_bonds = [[0] * self.bond_fdim]\n",
    "        self.a_scope = []\n",
    "        self.b_scope = []\n",
    "\n",
    "        a2b = [[]]\n",
    "        b2a = [0]\n",
    "        b2revb = [0]\n",
    "        for mol_graph in mol_graphs:\n",
    "            f_atoms.extend(mol_graph.f_atoms)\n",
    "            f_bonds.extend(mol_graph.f_bonds)\n",
    "\n",
    "            for a in range(mol_graph.n_atoms):\n",
    "                a2b.append([b + self.n_bonds for b in mol_graph.a2b[a]])\n",
    "\n",
    "            for b in range(mol_graph.n_bonds):\n",
    "                b2a.append(self.n_atoms + mol_graph.b2a[b])\n",
    "                b2revb.append(self.n_bonds + mol_graph.b2revb[b])\n",
    "\n",
    "            self.a_scope.append((self.n_atoms, mol_graph.n_atoms))\n",
    "            self.b_scope.append((self.n_bonds, mol_graph.n_bonds))\n",
    "            self.n_atoms += mol_graph.n_atoms\n",
    "            self.n_bonds += mol_graph.n_bonds\n",
    "            self.smiles_batch.append(mol_graph.smiles)\n",
    "\n",
    "        self.max_num_bonds = max(len(in_bonds) for in_bonds in a2b)\n",
    "\n",
    "        self.f_atoms = torch.FloatTensor(f_atoms)\n",
    "        self.f_bonds = torch.FloatTensor(f_bonds)\n",
    "        self.a2b = torch.LongTensor([a2b[a] + [0] * (self.max_num_bonds - len(a2b[a])) for a in range(self.n_atoms)])\n",
    "\n",
    "        self.b2a = torch.LongTensor(b2a)\n",
    "        self.b2revb = torch.LongTensor(b2revb)\n",
    "        self.b2b = None\n",
    "        self.a2a = None\n",
    "        \n",
    "\n",
    "    def get_components(self):\n",
    "        \"\"\"\n",
    "        Returns the components of the BatchMolGraph.\n",
    "        :return: A tuple containing PyTorch tensors with the atom features, bond features, and graph structure\n",
    "        and two lists indicating the scope of the atoms and bonds (i.e. which molecules they belong to).\n",
    "        \"\"\"\n",
    "        return self.f_atoms, self.f_bonds, self.a2b, self.b2a, self.b2revb, self.a_scope, self.b_scope\n",
    "\n",
    "    def get_b2b(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Computes (if necessary) and returns a mapping from each bond index to all the incoming bond indices.\n",
    "        :return: A PyTorch tensor containing the mapping from each bond index to all the incoming bond indices.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.b2b is None:\n",
    "            b2b = self.a2b[self.b2a]\n",
    "\n",
    "            revmask = (b2b != self.b2revb.unsqueeze(1).repeat(1, b2b.size(1))).long()\n",
    "            self.b2b = b2b * revmask\n",
    "\n",
    "        return self.b2b\n",
    "\n",
    "    def get_a2a(self) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Computes (if necessary) and returns a mapping from each atom index to all neighboring atom indices.\n",
    "        :return: A PyTorch tensor containing the mapping from each bond index to all the incodming bond indices.\n",
    "        \"\"\"\n",
    "        get_b2a = self.b2a.detach().numpy().tolist()\n",
    "\n",
    "        if self.a2a is None:\n",
    "\n",
    "            a2neia=[]\n",
    "            for incoming_bondIdList in self.a2b:\n",
    "                neia=[]\n",
    "                for incoming_bondId in incoming_bondIdList:\n",
    "                    neia.append(get_b2a[incoming_bondId])\n",
    "                a2neia.append(neia)\n",
    "            self.a2a=torch.LongTensor(a2neia)\n",
    "\n",
    "        return self.a2a\n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, smi_list, label_list):\n",
    "        \n",
    "        if not isinstance(smi_list, np.ndarray):\n",
    "            smi_list = smi_list.to_numpy()\n",
    "        if not isinstance(label_list, np.ndarray):\n",
    "            label_list = label_list.to_numpy()\n",
    "        \n",
    "        # Negative label is converted into 0 (originally -1)\n",
    "        if -1 in label_list:    \n",
    "            label_list[np.where(label_list==-1)[0]] = 0\n",
    "            \n",
    "        self.smi_list   = smi_list\n",
    "        self.label_list = label_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.label_list.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.smi_list[idx], self.label_list[idx]\n",
    "        #return [MolGraph(s) for s in self.smi_list[idx]], self.label_list[idx]\n",
    "  \n",
    "    \n",
    "def mycollate_fn(batch):\n",
    "    \n",
    "    batch_list = list(zip(batch))\n",
    "    mol_graphs = []\n",
    "    labels     = []\n",
    "    \n",
    "    for smi, label in batch:\n",
    "        mol_graphs.append(MolGraph(smi))\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels = torch.FloatTensor(labels).reshape([-1, 1])\n",
    "        \n",
    "    return mol_graphs, labels\n",
    "\n",
    "\n",
    "\n",
    "class MPNEncoder(nn.Module):\n",
    "    \"\"\"A message passing neural network for encoding a molecule.\"\"\"\n",
    "\n",
    "    def __init__(self, args: dict, weight_seed=1):\n",
    "        \"\"\"Initializes the MPNEncoder.\n",
    "        :param args: Arguments.\n",
    "        :param atom_fdim: Atom features dimension.\n",
    "        :param bond_fdim: Bond features dimension.\n",
    "        \"\"\"\n",
    "        torch.cuda.manual_seed(weight_seed)\n",
    "        torch.manual_seed(weight_seed)\n",
    "        random.seed(weight_seed)\n",
    "        np.random.seed(weight_seed)\n",
    "        \n",
    "        super(MPNEncoder,self).__init__()\n",
    "        \n",
    "        self.args     = args\n",
    "        self.act_func = nn.ReLU()\n",
    "        self.depth    = args['ConvNum']\n",
    "        self.dim      = int(args['dim'])\n",
    "        self.W_i      = nn.Linear(ATOM_FDIM, self.dim)\n",
    "        self.W_o      = nn.Linear(self.dim*2, self.dim)\n",
    "        \n",
    "        w_h_input_size = self.dim + BOND_FDIM\n",
    "        modulList      = [self.act_func, nn.Linear(w_h_input_size, self.dim)]\n",
    "        \n",
    "        for d in range(args['agg_depth']):\n",
    "            modulList.extend([self.act_func, nn.Linear(self.dim, self.dim)])\n",
    "       \n",
    "        for i in range(args['ConvNum']):\n",
    "            exec(f\"self.W_h{i} = nn.Sequential(*modulList)\")\n",
    "            \n",
    "        self_module = [nn.Linear(ATOM_FDIM, self.dim), self.act_func]\n",
    "        for d in range(args['agg_depth']):\n",
    "            self_module.extend([nn.Linear(self.dim, self.dim), self.act_func])\n",
    "            \n",
    "        self.W_ah = nn.Sequential(*self_module)\n",
    "\n",
    "\n",
    "    def forward(self, mol_graph):\n",
    "        \"\"\"\n",
    "        Encodes a batch of molecular graphs.\n",
    "        :param mol_graph: A BatchMolGraph representing a batch of molecular graphs.\n",
    "        :param features_batch: A list of ndarrays containing additional features.\n",
    "        :return: A PyTorch tensor of shape (num_molecules, hidden_size) containing the encoding of each molecule.\n",
    "        \"\"\"\n",
    "\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope = mol_graph.get_components()\n",
    "        a2a = mol_graph.get_a2a()\n",
    "        \n",
    "        if self.args['cuda']:\n",
    "            f_atoms, f_bonds, a2b, b2a, b2revb, a2a = f_atoms.cuda(), f_bonds.cuda(), a2b.cuda(), b2a.cuda(), b2revb.cuda(), a2a.cuda()\n",
    "            self.W_i, self.W_o = self.W_i.cuda(), self.W_o.cuda()\n",
    "            for i in range(self.depth-1):\n",
    "                exec(f\"self.W_h{i} = self.W_h{i}.cuda()\")\n",
    "                      \n",
    "        input = self.act_func(self.W_i(f_atoms))\n",
    "\n",
    "        self_message, message = input.clone(), input.clone()          \n",
    "        self_message[0, :], message[0, :] = 0, 0\n",
    "         \n",
    "        for depth in range(self.depth):\n",
    "            \n",
    "            nei_a_message, nei_f_bonds = index_select_ND(message, a2a), index_select_ND(f_bonds, a2b)\n",
    "            nei_message = torch.cat([nei_a_message, nei_f_bonds], dim=2)\n",
    "            message = nei_message.sum(dim=1).float()\n",
    "            \n",
    "            message = eval(f\"self.W_h{depth}(message)\")\n",
    "            message = self_message + message\n",
    "            self_message = message.clone()\n",
    "            message[0 , :] = 0\n",
    "        \n",
    "        nei_a_message = index_select_ND(message, a2a)\n",
    "        a_message = nei_a_message.sum(dim=1).float()\n",
    "        cc_message = self.W_ah(f_atoms)\n",
    "\n",
    "        a_input = torch.cat([cc_message, a_message], dim=1)\n",
    "        out = self.act_func(self.W_o(a_input))\n",
    "\n",
    "        mol_vecs = []\n",
    "        for i, (a_start, a_size) in enumerate(a_scope):\n",
    "            cur_hiddens = out.narrow(0, a_start, a_size)\n",
    "            mol_vec = cur_hiddens.sum(dim=0)\n",
    "            mol_vecs.append(mol_vec)\n",
    "        out = torch.stack(mol_vecs, dim=0)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, arg, random_seed=0):\n",
    "        \n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        hidden_list  = arg['node_list']\n",
    "        W_list       = [nn.Linear(hidden_list[0], hidden_list[1], bias=True)]\n",
    "        self.dropout = nn.Dropout(p=arg['dropout'])\n",
    "        self.active_function = nn.ReLU()\n",
    "        \n",
    "        for num in range(len(hidden_list)-2):\n",
    "            W_list.extend([self.active_function, self.dropout, nn.Linear(hidden_list[num+1], hidden_list[num+2], bias=True)])\n",
    "                \n",
    "        modulelist  =  nn.ModuleList(W_list) \n",
    "        self.W      =  nn.Sequential(*modulelist)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W(x)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN_CGR_Loaded(Base_wodirection_CGR):\n",
    "    \n",
    "    def __init__(self, target, modeltype, dir_log=None, dir_score=None, aconly=False, data_split_metric='trtssplit'):\n",
    "        super().__init__(target, modeltype, dir_log, dir_score, aconly, data_split_metric)\n",
    "        \n",
    "    def LoadModel(self, target):\n",
    "        p_args = './Log_wodirection_trtssplit_debug/MPNN/Models/params_%s_trial0.json' %target\n",
    "        p_mpn  = './Log_wodirection_trtssplit_debug/MPNN/Models/mpn_%s_trial0.pth' %target\n",
    "        p_dnn  = './Log_wodirection_trtssplit_debug/MPNN/Models/dnn_%s_trial0.pth' %target\n",
    "\n",
    "        self.args   = LoadJson(p_args)\n",
    "\n",
    "        self.mpn    = MPNEncoder(self.args)\n",
    "        #self.mpn.load_state_dict(torch.load(p_mpn))\n",
    "\n",
    "        self.dnn    = DeepNeuralNetwork(self.args)\n",
    "        #self.dnn.load_state_dict(torch.load(p_dnn))\n",
    "\n",
    "        self.cgr = self._ReadDataFile(target)\n",
    "        self._SetParams()\n",
    "\n",
    "    def GetInputData(self, trial):\n",
    "        tr, ts = self._GetTrainTest(trial)\n",
    "        trX, trY = tr[self.col], tr['class'].to_numpy()\n",
    "        tsX, tsY = ts[self.col], ts['class'].to_numpy()\n",
    "        \n",
    "        self.w_pos = int(np.where(trY==-1)[0].shape[0] / np.where(trY==1)[0].shape[0])\n",
    "        \n",
    "        return trX, trY, tsX, tsY\n",
    "    \n",
    "    def GetDataLoader(self, X, Y):\n",
    "        return DataLoader(Dataset(smi_list=X, label_list=Y),\n",
    "                                   batch_size = self.args['batch_size'],\n",
    "                                   shuffle    = False, \n",
    "                                   collate_fn = mycollate_fn\n",
    "                                   )\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \n",
    "        out  = self.mpn(X)\n",
    "        pred = self.dnn(out)\n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        out = self.output_act(X)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def test(self, dataloader, device='cuda'):\n",
    "        \n",
    "        self.mpn = self.mpn.cuda()\n",
    "        self.dnn = self.dnn.cuda()\n",
    "        \n",
    "        self.output_act = nn.Sigmoid()\n",
    "        self.output_act = self.output_act.cuda()\n",
    "        \n",
    "        self.mpn.eval()\n",
    "        self.dnn.eval()\n",
    "        \n",
    "        size = len(dataloader.dataset)\n",
    "        threshold = torch.tensor([0.5]).to(device)\n",
    "        test_loss, correct = 0, 0\n",
    "        pred_score_all, pred_all, proba_all, y_all = [], [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in dataloader:\n",
    "                X = BatchMolGraph(X)\n",
    "                # X, y = X.to(device), y.to(device)\n",
    "                pred_score  = self.predict(X)\n",
    "                proba       = self.predict_proba(pred_score)\n",
    "                pred        = (proba>threshold).float()*1\n",
    "                \n",
    "                pred_score_all += torch2numpy(pred_score).reshape(-1).tolist()\n",
    "                pred_all       += torch2numpy(pred).reshape(-1).tolist()\n",
    "                proba_all      += torch2numpy(proba).reshape(-1).tolist()\n",
    "                y_all          += torch2numpy(y).reshape(-1).tolist() \n",
    "        \n",
    "        return pred_score_all, pred_all, proba_all\n",
    "    \n",
    "    def train(self, args, device, dataloader, verbose=1):\n",
    "        \n",
    "        self.mpn.train()\n",
    "        self.dnn.train()\n",
    "        \n",
    "        \n",
    "        opt_list       = list(self.mpn.parameters()) + list(self.dnn.parameters())\n",
    "        self.optimizer = torch.optim.Adam(opt_list, lr=args['lr'])\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=args['step_size'], gamma=args['gamma'])\n",
    "\n",
    "        if args['cuda']:\n",
    "            self.mpn = self.mpn.cuda()\n",
    "            self.dnn = self.dnn.cuda()\n",
    "            \n",
    "        size = len(dataloader.dataset)\n",
    "        \n",
    "        n_usedtr = 0\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "              \n",
    "            # X, y = X.to(device), y.to(device)\n",
    "            X = BatchMolGraph(X)\n",
    "            \n",
    "            score = self.predict(X)\n",
    "            loss  =  self.loss_fn(score, y.to(device))\n",
    "            \n",
    "            # back propagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            #verbose\n",
    "            n_usedtr += y.shape[0]\n",
    "            if isinstance(verbose, int) & (batch % verbose == 0):\n",
    "                loss, current = loss.item(), batch * y.shape[0]\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                \n",
    "    def fit(self, args, trX, trY):\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([self.w_pos]))\n",
    "        self.loss_fn = self.loss_fn.cuda()\n",
    "        EPOCH = 50\n",
    "        \n",
    "        dataloader_tr = DataLoader(Dataset(smi_list=trX, label_list=trY),\n",
    "                                   batch_size = args['batch_size'],\n",
    "                                   shuffle    = True, \n",
    "                                   collate_fn = mycollate_fn\n",
    "                                   )\n",
    "        \n",
    "        self.mpn = MPNEncoder(args)\n",
    "        self.dnn = DeepNeuralNetwork(args)\n",
    "        for step in range(EPOCH):\n",
    "            self.train(args, device, dataloader_tr)\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.355231  [    0/  200]\n",
      "loss: 1.104961  [   72/  200]\n",
      "loss: 1.150591  [    0/  200]\n",
      "loss: 1.483739  [   72/  200]\n",
      "loss: 1.230686  [    0/  200]\n",
      "loss: 1.324672  [   72/  200]\n",
      "loss: 1.210477  [    0/  200]\n",
      "loss: 1.334115  [   72/  200]\n",
      "loss: 1.096521  [    0/  200]\n",
      "loss: 1.603630  [   72/  200]\n",
      "loss: 1.100452  [    0/  200]\n",
      "loss: 1.585685  [   72/  200]\n",
      "loss: 1.413250  [    0/  200]\n",
      "loss: 1.025847  [   72/  200]\n",
      "loss: 1.309012  [    0/  200]\n",
      "loss: 1.223434  [   72/  200]\n",
      "loss: 1.296677  [    0/  200]\n",
      "loss: 1.222243  [   72/  200]\n",
      "loss: 1.285259  [    0/  200]\n",
      "loss: 1.177093  [   72/  200]\n",
      "loss: 1.349337  [    0/  200]\n",
      "loss: 1.102341  [   72/  200]\n",
      "loss: 1.284372  [    0/  200]\n",
      "loss: 1.197602  [   72/  200]\n",
      "loss: 1.217959  [    0/  200]\n",
      "loss: 1.313411  [   72/  200]\n",
      "loss: 1.339818  [    0/  200]\n",
      "loss: 1.085901  [   72/  200]\n",
      "loss: 1.204735  [    0/  200]\n",
      "loss: 1.340854  [   72/  200]\n",
      "loss: 1.218036  [    0/  200]\n",
      "loss: 1.265664  [   72/  200]\n",
      "loss: 1.032806  [    0/  200]\n",
      "loss: 1.703191  [   72/  200]\n",
      "loss: 1.213791  [    0/  200]\n",
      "loss: 1.237248  [   72/  200]\n",
      "loss: 1.338819  [    0/  200]\n",
      "loss: 1.065573  [   72/  200]\n",
      "loss: 1.280372  [    0/  200]\n",
      "loss: 1.189750  [   72/  200]\n",
      "loss: 1.246942  [    0/  200]\n",
      "loss: 1.113997  [   72/  200]\n",
      "loss: 1.108814  [    0/  200]\n",
      "loss: 1.590055  [   72/  200]\n",
      "loss: 1.272087  [    0/  200]\n",
      "loss: 1.131218  [   72/  200]\n",
      "loss: 1.344637  [    0/  200]\n",
      "loss: 1.033994  [   72/  200]\n",
      "loss: 1.359687  [    0/  200]\n",
      "loss: 0.868733  [   72/  200]\n",
      "loss: 1.198822  [    0/  200]\n",
      "loss: 1.163370  [   72/  200]\n",
      "loss: 1.159293  [    0/  200]\n",
      "loss: 1.395125  [   72/  200]\n",
      "loss: 1.275343  [    0/  200]\n",
      "loss: 0.993876  [   72/  200]\n",
      "loss: 1.303880  [    0/  200]\n",
      "loss: 1.031952  [   72/  200]\n",
      "loss: 1.343681  [    0/  200]\n",
      "loss: 0.853023  [   72/  200]\n",
      "loss: 0.996198  [    0/  200]\n",
      "loss: 1.342431  [   72/  200]\n",
      "loss: 1.305046  [    0/  200]\n",
      "loss: 0.789684  [   72/  200]\n",
      "loss: 1.381561  [    0/  200]\n",
      "loss: 1.177541  [   72/  200]\n",
      "loss: 1.095282  [    0/  200]\n",
      "loss: 1.233774  [   72/  200]\n",
      "loss: 1.050045  [    0/  200]\n",
      "loss: 1.163942  [   72/  200]\n",
      "loss: 1.137186  [    0/  200]\n",
      "loss: 1.143528  [   72/  200]\n",
      "loss: 1.233908  [    0/  200]\n",
      "loss: 0.912688  [   72/  200]\n",
      "loss: 0.955934  [    0/  200]\n",
      "loss: 1.259869  [   72/  200]\n",
      "loss: 1.008415  [    0/  200]\n",
      "loss: 1.295735  [   72/  200]\n",
      "loss: 1.640725  [    0/  200]\n",
      "loss: 0.970510  [   72/  200]\n",
      "loss: 1.098142  [    0/  200]\n",
      "loss: 1.187917  [   72/  200]\n",
      "loss: 1.008855  [    0/  200]\n",
      "loss: 1.266005  [   72/  200]\n",
      "loss: 1.281476  [    0/  200]\n",
      "loss: 0.845397  [   72/  200]\n",
      "loss: 1.350583  [    0/  200]\n",
      "loss: 0.631277  [   72/  200]\n",
      "loss: 1.476169  [    0/  200]\n",
      "loss: 0.981244  [   72/  200]\n",
      "loss: 1.171806  [    0/  200]\n",
      "loss: 0.790656  [   72/  200]\n",
      "loss: 1.539399  [    0/  200]\n",
      "loss: 1.176433  [   72/  200]\n",
      "loss: 1.159827  [    0/  200]\n",
      "loss: 0.817497  [   72/  200]\n",
      "loss: 1.584137  [    0/  200]\n",
      "loss: 1.333308  [   72/  200]\n",
      "loss: 1.075032  [    0/  200]\n",
      "loss: 1.572921  [   72/  200]\n"
     ]
    }
   ],
   "source": [
    "model  = \"MPNN\"\n",
    "mtype  = \"wodirection_trtssplit_debug\"\n",
    "target = 'CHEMBL4072'\n",
    "trial  = 2\n",
    "\n",
    "model = MPNN_CGR_Loaded(target=target, modeltype  = mtype)\n",
    "model.LoadModel(target=target)\n",
    "#model.args['lr'] = 0.005\n",
    "#model.args['node_list'] = [100, 60, 20, 1]\n",
    "trX, trY, tsX, tsY = model.GetInputData(trial=trial)\n",
    "\n",
    "model.fit(model.args, trX, trY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.GetDataLoader(trX, trY)\n",
    "\n",
    "pred_score_all, pred_all, proba_all = model.test(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_num': 50,\n",
       " 'batch_size': 128,\n",
       " 'agg_depth': 1,\n",
       " 'cuda': True,\n",
       " 'gamma': 0.1,\n",
       " 'ConvNum': 3,\n",
       " 'dropout': 0.2,\n",
       " 'dim': 70.0,\n",
       " 'step_num': 3,\n",
       " 'DNNLayerNum': 8,\n",
       " 'lr': 0.0006461389804230061,\n",
       " 'step_size': 16,\n",
       " 'grad_node': 8,\n",
       " 'node_list': [70, 62, 54, 46, 38, 30, 22, 14, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  13,  16,\n",
       "         18,  19,  20,  21,  24,  26,  27,  28,  29,  31,  32,  33,  36,\n",
       "         37,  39,  40,  43,  44,  47,  48,  49,  50,  52,  55,  56,  58,\n",
       "         61,  62,  63,  64,  65,  66,  69,  71,  73,  74,  75,  76,  78,\n",
       "         83,  84,  85,  86,  88,  89,  90,  91,  92,  93,  94,  95,  97,\n",
       "         98,  99, 100, 105, 107, 110, 112, 113, 114, 115, 119, 120, 122,\n",
       "        126, 127, 128, 129, 130, 131, 133, 135, 137, 139, 140, 141, 142,\n",
       "        145, 147, 148, 150, 153, 154, 155, 156, 158, 160, 161, 162, 164,\n",
       "        165, 166, 168, 169, 170, 171, 172, 173, 175, 179, 180, 181, 182,\n",
       "        185, 186, 187, 188, 189, 190, 191, 192, 194, 195]),)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.array(pred_all)==1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 35, 40, 75, 81, 83]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(tsY==1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [87, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7856e9550f37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproba_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    543\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    544\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         return _average_binary_score(partial(_binary_roc_auc_score,\n",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     fpr, tpr, _ = roc_curve(y_true, y_score,\n\u001b[0;32m--> 331\u001b[0;31m                             sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    332\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmax_fpr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \"\"\"\n\u001b[1;32m    913\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 914\u001b[0;31m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rdkit/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 320\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [87, 200]"
     ]
    }
   ],
   "source": [
    "roc_auc_score(tsY, proba_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06887408268302096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(tsY, pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f7d60dd648d82e5ef4c4f9ba1946e62b6642da77143b0b983cfb14b8b57c97a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('rdkit': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
